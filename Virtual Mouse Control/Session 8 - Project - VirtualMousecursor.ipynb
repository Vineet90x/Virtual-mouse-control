{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SESSION 8 - Project Virtual Mouse Cursor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOC : <br>\n",
    "\n",
    "1. Introduction\n",
    "2. Importing Packages\n",
    "3. Project Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the necessary packages and Convention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "The mouse is the one of the wonderfull inventions of the Human Computer Interaction(HCI) Technology.\n",
    "\n",
    "Currently we are using wired or may be wireless mouses,In real time cases some computers may not support for a physical mouse or may some users may be dealth with some hand problems or handicap and cannot use physical mouse, so this Hand controlled AI Virtual Mouse can be used to overcome this problem. Making a user to control the mouse by reducing the computer human interaction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNCTIONALITIES:\n",
    "\n",
    "- This was built using the openCV-python and mediapipe for Detecting and processing the image and mediapipe an open source cross-platform developed by google for media processing and ready-to-use ML solutions for computer vision tasks.\n",
    "- Our Hand Controlled Virtual mouse can able to move the mouse anywhere on the screen and can able to perform the click operation.\n",
    "- Our Index finger can be used to move the mouse over the screen.\n",
    "- When Our Index finger and Thumb come close to each other or touch each other then it performs the click operation.\n",
    "- And the PyautoGUI for programmatically control the mouse and keyboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install the following necessary pip\n",
    "```\n",
    "- pip install opencv-python\n",
    "- pip install numpy\n",
    "- pip install matplotlib\n",
    "- pip install mediapipe\n",
    "- pip install PyautoGUI\n",
    "- pip install PIL\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import ht\n",
    "import cv2  # pip install opencv-contrib-python\n",
    "import numpy as np\n",
    "import mediapipe as mp  # pip install mediapipe\n",
    "import pyautogui    # pip install PyautoGUI\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/hand_landmarks.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = Image(url=\"images/hand_landmarks.png\")\n",
    "points\n",
    "# img=cv2.imread(\"hand_landmarks.png\")\n",
    "# RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)   # capture video '0' one cam\n",
    "hand_detector = mp.solutions.hands.Hands()  # detect hand\n",
    "drawing_utils = mp.solutions.drawing_utils\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "# print(screen_width, screen_height)\n",
    "index_y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Smoothen the movement of mouse to stop at the exact position of,\n",
    "   our hand movement without any shake in the movement of the mouse'''\n",
    "smoothening = 9\n",
    "plocx, plocy = 0, 0\n",
    "clocx, clocy = 0, 0 \n",
    "x1,y1 = 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  60.75\n",
      "click\n",
      "distance :  348.75\n",
      "distance :  155.25\n",
      "distance :  364.5\n",
      "distance :  459.0\n",
      "distance :  456.75\n",
      "distance :  418.5\n",
      "distance :  425.25\n",
      "distance :  501.75\n",
      "distance :  465.75\n",
      "distance :  436.5\n",
      "distance :  434.25\n",
      "distance :  396.0\n",
      "distance :  398.25\n",
      "distance :  380.25\n",
      "distance :  405.0\n",
      "distance :  303.75\n",
      "distance :  357.75\n",
      "distance :  380.25\n",
      "distance :  207.0\n",
      "distance :  362.25\n",
      "distance :  355.5\n",
      "distance :  324.0\n",
      "distance :  315.0\n",
      "distance :  326.25\n",
      "distance :  315.0\n",
      "distance :  308.25\n",
      "distance :  315.0\n",
      "distance :  310.5\n",
      "distance :  315.0\n",
      "distance :  321.75\n",
      "distance :  317.25\n",
      "distance :  315.0\n",
      "distance :  209.25\n",
      "distance :  184.5\n",
      "distance :  380.25\n",
      "distance :  371.25\n",
      "distance :  402.75\n",
      "distance :  432.0\n",
      "distance :  400.5\n",
      "distance :  423.0\n",
      "distance :  324.0\n",
      "distance :  364.5\n",
      "distance :  450.0\n",
      "distance :  409.5\n",
      "distance :  423.0\n",
      "distance :  99.0\n",
      "distance :  400.5\n",
      "distance :  355.5\n",
      "distance :  393.75\n",
      "distance :  405.0\n",
      "distance :  371.25\n",
      "distance :  371.25\n",
      "distance :  364.5\n",
      "distance :  369.0\n",
      "distance :  74.25\n",
      "distance :  342.0\n",
      "distance :  135.0\n",
      "distance :  337.5\n",
      "distance :  9.0\n",
      "click\n",
      "distance :  389.25\n",
      "distance :  58.5\n",
      "click\n",
      "distance :  371.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m frame_height, frame_width, _ \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      8\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# detect on rgb frame color\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mhand_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m hands \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks \u001b[38;5;66;03m# hand landmark\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hands:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _, frame = cap.read()   # read data from cap\n",
    "    '''Flip the frame or screen since the camera shows the mirror image,\n",
    "       of our hand and moves in opposite direction so we need to flip the screen'''\n",
    "    frame = cv2.flip(frame, 1)\n",
    "     # shape gives frame height and width using shape \n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # detect on rgb frame color\n",
    "    output = hand_detector.process(rgb_frame)\n",
    "    hands = output.multi_hand_landmarks # hand landmark\n",
    "    \n",
    "    if hands:\n",
    "        for hand in hands:\n",
    "            drawing_utils.draw_landmarks(frame, hand)   # see landmarks on frame\n",
    "            landmarks = hand.landmark\n",
    "            # we use our index finger tip move the mouse \n",
    "    \n",
    "            for id, landmark in enumerate(landmarks):   # add counter\n",
    "                # show the landmarks on kernel in x and y axis\n",
    "                # x and y axis is multiplies by the height and width to get the x and y axis on the frames\n",
    "                x = int(landmark.x*frame_width)\n",
    "                y = int(landmark.y*frame_height)\n",
    "                # Index finger tip point number is 8\n",
    "                # and draw a boundary to the point a circle\n",
    "                if id == 8:\n",
    "                    cv2.circle(img=frame, center=(x,y), radius=15, color=(0, 255, 255))\n",
    "                    # pyautogui.moveTo(x,y)\n",
    "                    index_x = (screen_width/frame_width)*x\n",
    "                    index_y = (screen_height/frame_height)*y\n",
    "                    # co-ordinates need to be changed \n",
    "                    # smoothining varies with the change in the smoothening factor\n",
    "                    clocx = plocx + ( index_x - plocx ) / smoothening\n",
    "                    clocy = plocy + ( index_y - plocy ) / smoothening\n",
    "                    pyautogui.moveTo(clocx, clocy)\n",
    "                    plocx, plocy, x1, y1 = clocx, clocy, landmark.x, landmark.y\n",
    "                \n",
    "                # thumb tip point number is 4\n",
    "\n",
    "                if id == 4:\n",
    "                    cv2.circle(img=frame, center=(x,y), radius=15, color=(0, 255, 255))\n",
    "                    thumb_x = (screen_width/frame_width)*x\n",
    "                    thumb_y = (screen_height/frame_height)*y\n",
    "                    print('distance : ', abs(index_y - thumb_y))\n",
    "                    if abs(index_y - thumb_y) < 70:\n",
    "                        print('click')\n",
    "                        pyautogui.click()\n",
    "                        pyautogui.sleep(1)\n",
    "    cv2.imshow('Virtual Mouse', frame)  # show image\n",
    "    #cv2.waitKey(1)  # waits for key infinitely\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkout the Demo Below :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align = \"middle\" src = 'images/virtual mouse.gif'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "fa6820bafbacf8424655be4f036d97fe1bf53e85f3476c591555871e686f57fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
